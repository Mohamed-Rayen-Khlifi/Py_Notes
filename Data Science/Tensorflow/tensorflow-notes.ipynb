{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"### Framework for implementing deep learning algorithms\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Normalization # Import three layers from keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\nfrom tensorflow.keras.activations import sigmoid\n\nimport numpy as np","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data in TensorFlow\n#### By represeting data as matrix it helps tf be more computationally efficient internally","metadata":{}},{"cell_type":"code","source":"General_Matrix = np.array([[col1, col2], # Row1\n                           [col1, col2], # Row2\n                           [col1, col2], # Row3\n                           [col1, col2]  # Row4\n                          ])\n\n# 4x2 Matrix\nX = np.array([[200.0, 17.0], \n              [120.0, 5.0],\n              [425.0, 20.0],\n              [212.0, 18.0]]) \n\n\n# 1d array = list of numbers and not a matrix \ny = np.array([1, 0, 0, 1])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalize Data","metadata":{}},{"cell_type":"code","source":"### Fitting the weights to the data (back-propagation) will proceed more quickly if the data features are normalized (Have similar range)\n\n# Create a normalization layer (Not a layer in your model)\nnorm_layer = tf.keras.layers.Normalization(axis=-1)\n\n# Adapt the data: Learn the mean and variance of the data set and save the values internally\nnorm_layer.adapt(X)  \n\n# Normalize the data (Apply this to any future data that utilizes the learned model)\nXn = norm_layer(X)","metadata":{"execution":{"iopub.status.busy":"2024-06-27T10:16:39.086404Z","iopub.execute_input":"2024-06-27T10:16:39.086835Z","iopub.status.idle":"2024-06-27T10:16:39.093856Z","shell.execute_reply.started":"2024-06-27T10:16:39.086802Z","shell.execute_reply":"2024-06-27T10:16:39.092568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference (Forward Propagation) in NN ","metadata":{}},{"cell_type":"markdown","source":"### Explicitly one layer of computation at a time ","metadata":{}},{"cell_type":"code","source":"layer_1 = tf.keras.layers.Dense(units = 25, activation='sigmod')\na1 = layer_1(X) #a1 is a 25x1 matrix (a1 is a tensor)\n\nlayer_2 = tf.keras.layers.Dense(units = 15, activation='sigmod')\na2 = layer_1(a1)\n\n# Output Layer with 1 neuron\nlayer_3 = tf.keras.layers.Dense(units = 1, activation='sigmod')\na3 = layer_1(a2) #Output is 2d array that is a 1x1 matrix","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Sequential to string the layers and make a NN","metadata":{}},{"cell_type":"code","source":"layer_1 = tf.keras.layers.Dense(units = 25, activation='sigmod')\nlayer_2 = tf.keras.layers.Dense(units = 15, activation='sigmod')\nlayer_3 = tf.keras.layers.Dense(units = 1, activation='sigmod')\n\nmodel = Sequential([layer_1, layer_2, layer_1])  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Formal way\n#### Tensorflow models are built layer by layer, a layer's input dimensions are calculated for you. You specify a layer's output dimensions and this determines the next layer's input dimension.","metadata":{}},{"cell_type":"code","source":"# Like random_state\ntf.random.set_seed(1234) \n\n# The Sequential model is a convenient means of constructing multi-layer models\nmodel = Sequential(\n    [        \n    # Input Layer and shape specifies the expected shape of the input\n    # Allows Tensorflow to size the weights and bias parameteress at this points given the shape of the input\n    # This statement can be omitted in practice and Tensorflow will size the network parameters when the input data is specified in the \"model.fit\" statement \n        \n    tf.keras.Input(shape=(1,m)), #Input is always a one row MATRIX with multiple columns (1,m)\n    Dense(units = 25, activation='sigmod', name='first_layer'),\n    Dense(units = 15, activation='sigmod', name='second_layer'),\n    Dense(units = 1, name='output_layer') # Dont include sigmmoid activation in the final layer its accounted for in the loss\n    ], name = \"my_model\"\n)\n\n# Defines a loss function and specifies a compile optimization\nmodel.compile(\n    loss = tf.keras.losses.BinaryCrossentropy(),\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n)\n\n# Runs gradient descent to fit the weights to the data\n# For efficiency, the training data set is broken into 'batches' with a default size of 32 \n# epochs specifies how many times the NN will pass through the entire training dataset during training\n# Each epoch represents one full cycle through all the training data.\nmodel.fit(X, y, \n          batch_size=32, \n          epochs=10,)\n\n\nX_new = np.array([[350.0, 122.0]])\n\n# Carries out forward propagation using the NN we compiled using Sequential\n# Returns a matrix 1x1 carrying the probability\npredictions = model.predcit(X_new) \n\n\n# Apply a threshold to convert the probabilities to a decision\nyhat = np.zeros_like(predictions)\nfor i in range(len(predictions)):\n    if predictions[i] >= 0.5:\n        yhat[i] = 1\n    else:\n        yhat[i] = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utilities on layers \n#### Number of parameters per layer = (number_of_units x number_of_input_features) + number_of_units  \n\n#### -The weights ùëä should be a matrix with a size (number_of_input_features, number_of_units) \n#### -The bias ùëè size should be a 1d array with a size equal to the (number_of_units)\n\n#### -Concatenate every parameter vector (column) of each neuron in the layer to create the ùëä matrix that has all the parameters\n#### -Same approach with the bias vector that can be represented as a 1-D (n,) or 2-D (1,n) array but Tensorflow utilizes a 1-D representation)","metadata":{}},{"cell_type":"code","source":"# Shows the layers and number of parameters in the model\n# The parameter counts shown in the summary correspond to the number of elements in the weight and bias arrays\nmodel.summary()\n\n# Extract the layers\n[layer1, layer2, layer3] = model.layers\n\n# Extract the weights of each layer\nW1,b1 = layer1.get_weights()\nW2,b2 = layer2.get_weights()\nW3,b3 = layer3.get_weights()\n\nfirst_layer = model.get_layer('first_layer')\nw,b = first_layer.get_weights()\n\n# Set the weights\nset_b = np.array([100, ....])\nset_w = np.array([[200, ....]])\nfirst_layer.set_weights([set_w, set_b])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utilities on tensors","metadata":{}},{"cell_type":"code","source":"# Convert the tensor to a numpy array\na1.numpy()\n\n#Access weights directly in their tensor form\nmodel.layers[2].weights ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neuron without activation - Regression/Linear Model","metadata":{}},{"cell_type":"code","source":"X_train = np.array([[1.0], [2.0]], dtype=np.float32)           #(size in 1000 square feet)\nY_train = np.array([[300.0], \n                    [500.0]], dtype=np.float32)       #(price in 1000s of dollars)\n\n# A layer with one neuron or unit \nlinear_layer = tf.keras.layers.Dense(units=1, activation = 'linear', )\n\n# Prediction\nprediction_tf = linear_layer(X_train)\nprediction_np = np.dot( X_train, set_w) + set_b\n\nprediction_tf == prediction_np #True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neuron with Sigmoid activation","metadata":{}},{"cell_type":"code","source":"X_train = np.array([0., 1, 2, 3, 4, 5], dtype=np.float32).reshape(-1,1)  # 2-D Matrix\nY_train = np.array([0,  0, 0, 1, 1, 1], dtype=np.float32).reshape(-1,1)  # 2-D Matrix\n\nmodel =  tf.keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')\n\n# Prediction\nprediction = model.predict(X_train[0].reshape(1,1))\nalog = sigmoidnp(np.dot(set_w,X_train[0].reshape(1,1)) + set_b)\n\na1 == alog #True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Dense and Sequential from Scratch","metadata":{}},{"cell_type":"code","source":"# Building dense with and without vectorisation\ndef my_dense(a_in, W, b, g):\n    units = W.shape[1]\n    a_out = np.zeros(units)\n    for j in range(units):               \n        w = W[:,j]                                    \n        z = np.dot(w, a_in) + b[j]         \n        a_out[j] = g(z)               \n    return(a_out)\n\ndef my_dense_vectorized(A_in, W, B, g): #Everything is a matrix \n    Z = np.matmult(A_in, W) + B\n    A_out = g(Z)\n    return A_out\n\n# Building the predict function\ndef my_predict(X, W1, b1, W2, b2):\n    m = X.shape[0]\n    p = np.zeros((m,1))\n    for i in range(m):\n        p[i,0] = my_sequential(X[i], W1, b1, W2, b2)\n    return(p)\n\n# Preparing some parameters instead of building the gradient descent function\nW1_tmp = np.array( [[-8.93,  0.29, 12.9 ], \n                    [-0.1,  -7.32, 10.81]] )\nb1_tmp = np.array( [-9.82, -9.28,  0.96] )\n\nW2_tmp = np.array( [[-31.18], \n                    [-27.59], \n                    [-32.56]] )\nb2_tmp = np.array( [15.41] )\n\n\n# Preparing some test data\nX_tst = np.array([\n    [200,13.9],  # postive example\n    [200,17]])   # negative example\n\n# Normalizing the test data\nX_tstn = norm_l(X_tst) \n\n# Prediction (Between 0 and 1)\npredictions = my_predict(X_tstn, W1_tmp, b1_tmp, W2_tmp, b2_tmp)\n\n# Threshold application\nyhat = np.zeros_like(predictions)\nfor i in range(len(predictions)):\n    if predictions[i] >= 0.5:\n        yhat[i] = 1\n    else:\n        yhat[i] = 0","metadata":{"execution":{"iopub.status.busy":"2024-06-28T18:21:13.272787Z","iopub.execute_input":"2024-06-28T18:21:13.273215Z","iopub.status.idle":"2024-06-28T18:21:13.280120Z","shell.execute_reply.started":"2024-06-28T18:21:13.273184Z","shell.execute_reply":"2024-06-28T18:21:13.278865Z"},"trusted":true},"execution_count":2,"outputs":[]}]}