{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Mean Absolute Error (MAE)","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\nprediction = model.predict(X)\n\n### On average, our predictions are off by\nmean_absolute_error(y, prediction)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split Data\n##### The split is based on a random number generator, Supplying a numeric value to the random_state argument guarantees we get the same split every time we run this script","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split data into training and validation data for both features and target\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, scale\nfrom sklearn.linear_model import SGDRegressor\n\nscaler = StandardScaler()\n\n### Z Scale the training data\nX_norm = scaler.fit_transform(X_train) \n\n### Another way to scale generally with options\nscale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)\n\n### Scikit-learn has a gradient descent regression model that performs best with normalized input\nsgdr = SGDRegressor(max_iter=1000)\nsgdr.fit(X_norm, y_train)\n\n# Make predictions with normalized data\ny_pred_sgd = sgdr.predict(X_norm)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlinear_model = LinearRegression()\n    \n# X must be a 2-D Matrix\nlinear_model.fit(X_train.reshape(-1, 1), y_train) \n\n# View parameters\nb = linear_model.intercept_\nw = linear_model.coef_\n\n# Make predictions\ny_pred = linear_model.predict(X_train.reshape(-1, 1))\n\n# Accuracy\nlinear_model.score(X_train.reshape(-1,1), y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression()\nlr_model.fit(X, y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree","metadata":{}},{"cell_type":"code","source":"# Code you have previously used to load data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\n\n# Path of the file to read\niowa_file_path = '../input/home-data-for-ml-course/train.csv'\n\nhome_data = pd.read_csv(iowa_file_path)\n\n\n# Create target object and call it y\ny = home_data.SalePrice\n\n# Create X\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = home_data[features]\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\n# Specify Model\niowa_model = DecisionTreeRegressor(max_leaf_nodes = 20, random_state=1)\n\n# Fit Model\niowa_model.fit(train_X, train_y)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\n### Controlling Tree Depth\n# The max_leaf_nodes argument provides a very sensible way to control overfitting vs underfitting \n# The more leaves we allow the model to make, the more we move towards the overfitting\n# We can use a utility function to help compare MAE scores from different values for max_leaf_nodes\n\ndef get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree\n# It generally has much better predictive accuracy than a single decision tree and it works well with default parameters \n# But one of the best features of Random Forest models is that they generally work reasonably even without any tuning\n\nforest_model = RandomForestRegressor(random_state=1)\n\nforest_model.fit(train_X, train_y)","metadata":{},"execution_count":null,"outputs":[]}]}